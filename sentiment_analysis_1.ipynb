{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sg2083/independent_study/blob/main/sentiment_analysis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Stock-Related News & Posts and Predicting Stock Market Prices\n",
        "\n",
        "### Introduction\n",
        "The stock market is highly influenced by investor sentiment, which is often reflected in news articles, social media discussions, and online forums. This study focuses on analyzing the sentiment of stock-related posts from multiple sources, including Reddit, NewsAPI, and historical stock prices.\n",
        "\n",
        "The goal is to determine whether online sentiment correlates with stock price movements and if it can be used as a predictive feature for stock performance.\n",
        "\n",
        "## Research Questions\n",
        "1. Does investor sentiment expressed in Reddit posts and news articles correlate with stock price movements for Tesla?\n",
        "2. Can sentiment data extracted from online platforms be used to predict stock price trends in the short term?\n",
        "3. What is the relative significance of different sentiment sources (Reddit vs. NewsAPI) in predicting stock market performance?\n",
        "4. How does sentiment change in response to major news events, and does this sentiment shift correlate with subsequent stock price movements?\n",
        "\n",
        "##Hypotheses\n",
        ">**H1**:There is a significant positive correlation between positive sentiment in Reddit posts/news articles and an increase in Tesla's stock price.<br>\n",
        "**H2**: Negative sentiment in Reddit posts/news articles is significantly correlated with a decrease in Tesla's stock price.<br>\n",
        "**H3**: Sentiment data from Reddit is more predictive of short-term stock price fluctuations than sentiment data from news articles.<br>\n",
        "**H4**: Major news events (e.g., product launches, regulatory announcements) cause a significant shift in sentiment, which is reflected in short-term stock price movements.\n",
        "\n",
        "## Literature Review\n",
        "The study by Nti, Adekoya, and Weyori (2020) investigates how public sentiment, derived from web news, Twitter, Google Trends, and forum discussions, influences stock market predictions. Using sentiment analysis with an Artificial Neural Network (ANN) model, the authors predict stock prices on the Ghana Stock Exchange (GSE) over time frames of 1 to 90 days. They find that combining multiple data sources improves prediction accuracy, with the highest accuracy (70.66–77.12%) achieved from a combined dataset. The study highlights a strong link between stock market behavior and social media, suggesting that sentiment data from online platforms can help investors predict future stock price movements and make better investment decisions.\n",
        "link: https://sciendo.com/article/10.2478/acss-2020-0004\n",
        "\n",
        "## How its different from whats already been done\n",
        "Event-Driven Sentiment Evolution and Its Impact on Stock Price Prediction\n",
        "\n",
        "### Data\n",
        "The data for this study is collected from three primary sources: **Reddit, NewsAPI, and stock market data**. Reddit posts related to **Tesla** stock are retrieved using praw library from financial discussion subreddits like r/wallstreetbets, capturing post titles and timestamps. News articles mentioning Tesla are obtained via NewsAPI, extracting headlines, publication dates, and sources. Historical stock price data is being sourced from Yahoo Finance api, including daily open, high, low, close prices, trading volume, and other financial indicators.\n",
        "\n",
        "Since these datasets originate from different platforms, they contain varying timestamp formats, time zones, and missing values, requiring careful preprocessing and merging to align sentiment data with stock price movements for further analysis.\n",
        "\n",
        "### Data Preprocessing\n",
        "The collected data is being cleaned and standardized before merging. Steps include:\n",
        "\n",
        "1. Date Format Standardization\n",
        "\n",
        "  * Convert timestamps from different time zones to UTC\n",
        "  * Convert stock market timestamps (which include hours/minutes) to date-only format\n",
        "\n",
        "2. Column Renaming for Clarity\n",
        "\n",
        "  * Title → title_reddit (for Reddit)\n",
        "  * Title → title_news (for NewsAPI)\n",
        "  \n",
        "  This prevents column name conflicts\n",
        "\n",
        "3. Handling Missing Data\n",
        "\n",
        "  * Some dates lack both Reddit posts and news articles\n",
        "  * Missing values must be carefully handled to avoid bias\n",
        "\n",
        "4. Merging Data\n",
        "\n",
        "  Outer join used to keep all records from Reddit, NewsAPI, and stock price data Ensures no loss of important data points Note: Since data comes from multiple sources, preprocessing is still in progress to handle scattered and missing data."
      ],
      "metadata": {
        "id": "ecYuUm5IMIkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Importing required libraries\n",
        "!pip install newsapi-python\n",
        "!pip install praw\n",
        "\n",
        "import yfinance as yf\n",
        "from newsapi import NewsApiClient\n",
        "import praw\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uCIzjy_yYoZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff64a91-e9e8-4f74-950c-f175dea813f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from newsapi-python) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2025.1.31)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNqQY7A8PVlk",
        "outputId": "d0a34624-81b5-4c8a-a82a-294687468a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 Open        High         Low       Close  \\\n",
            "Date                                                                        \n",
            "2024-02-12 00:00:00-05:00  192.110001  194.729996  187.279999  188.130005   \n",
            "2024-02-13 00:00:00-05:00  183.990005  187.259995  182.110001  184.020004   \n",
            "2024-02-14 00:00:00-05:00  185.300003  188.889999  183.350006  188.710007   \n",
            "2024-02-15 00:00:00-05:00  189.160004  200.880005  188.860001  200.449997   \n",
            "2024-02-16 00:00:00-05:00  202.059998  203.169998  197.399994  199.949997   \n",
            "\n",
            "                              Volume  Dividends  Stock Splits  \n",
            "Date                                                           \n",
            "2024-02-12 00:00:00-05:00   95498600        0.0           0.0  \n",
            "2024-02-13 00:00:00-05:00   86759500        0.0           0.0  \n",
            "2024-02-14 00:00:00-05:00   81203000        0.0           0.0  \n",
            "2024-02-15 00:00:00-05:00  120831800        0.0           0.0  \n",
            "2024-02-16 00:00:00-05:00  111173600        0.0           0.0  \n"
          ]
        }
      ],
      "source": [
        "# @title Fetching stock history data for Tesla stocks\n",
        "tesla = yf.Ticker(\"TSLA\")\n",
        "tesla_data = tesla.history(period=\"1y\")\n",
        "print(tesla_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tweepy\n",
        "\n",
        "# # Replace with your API keys\n",
        "# api_key = \"QJDjqRmClnqkLg7nmNQ2gg9Qc\"\n",
        "# api_secret = \"kzuVz5zut9BdrbUML9w3upnwkW7mmFWe7iLUnxdsiBRU10w3ec\"\n",
        "# access_token = \"1268163737153728512-Qr5jw5gZ6mz2ZbN4QW3pq5kxw5JAi7\"\n",
        "# access_secret = \"qQ5WVKhmj3XX17WQxMfoJEzgAqwFxwz8qdYEsS3IaL69a\"\n",
        "# bearer_token = \"AAAAAAAAAAAAAAAAAAAAAMMezAEAAAAAtN2XctoVzTlyEi8YhnI6%2FEeIRUM%3Dx7uIiCT3lQMTLkCSJkAGPBVIqawsEkrH4qXPlWJNOC2JYupBwa\"\n",
        "\n",
        "# # Authenticate\n",
        "# client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import API\n",
        "\n",
        "consumer_key = 'YFUHQYil2JuiR4ws600kN2yD2'\n",
        "consumer_secret = '0x6LEfbpBz0Rnvm3dBw7lFAeygUTFcDaNU4YjN6eUMTFHLoUP9'\n",
        "access_token = '1889172540888948736-jrvTNNgPQHvEfE5OAyaaCBBfx2xpWR'\n",
        "access_token_secret = 'dK2tcEBm1nEBxZoGAiZw2TgnDZUhHyfMuYzVGrlorekeN'\n",
        "# Consumer key authentication\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "\n",
        "# Access key authentication\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Set up the API with the authentication handler\n",
        "api = API(auth)"
      ],
      "metadata": {
        "id": "3EZXpQ4MhuvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import csv\n",
        "import datetime\n",
        "\n",
        "# Twitter API credentials\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAFw2zAEAAAAABxOmWd%2BogWMxVgvfWVX3Lrsy8T4%3D9QG2pYsqHFgF2XkzGbC7oiIALJNQGE13bI9uv60b0i5oPJWWdo'  # Bearer Token for API v2\n",
        "\n",
        "# Authenticate to the Twitter API\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# Define the query and date range for historic tweets\n",
        "query = 'Tesla'\n",
        "# Use 'since' and 'until' as parameters to filter for tweets within the past 7 days\n",
        "max_tweets = 10  # Limit to 10 tweets\n",
        "\n",
        "# Create a function to collect the tweets\n",
        "def collect_tweets(query, max_tweets=10):\n",
        "    tweets = []\n",
        "    # Using search_recent_tweets for recent tweets (within the past 7 days)\n",
        "    for tweet in tweepy.Paginator(client.search_recent_tweets,  # Using search_recent_tweets for free access\n",
        "                                  query=query,\n",
        "                                  tweet_fields=['created_at', 'author_id', 'text'],\n",
        "                                  max_results=10).flatten(limit=max_tweets):  # Limit to 10 tweets\n",
        "        tweets.append([tweet.created_at, tweet.author_id, tweet.text])\n",
        "\n",
        "    return tweets\n",
        "\n",
        "# Collect the tweets\n",
        "tweets = collect_tweets(query, max_tweets)\n",
        "\n",
        "# Save the tweets to a CSV file\n",
        "with open('tesla_tweets.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Date\", \"User ID\", \"Tweet\"])\n",
        "    writer.writerows(tweets)\n",
        "\n",
        "print(f\"Collected {len(tweets)} tweets about Tesla.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GZQkoFZcEiz",
        "outputId": "e147149c-4ecc-44cb-9f77-aea1529c980b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 10 tweets about Tesla.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"\\n--- CSV Content ---\")\n",
        "with open('tesla_tweets.csv', 'r', encoding='utf-8') as file:\n",
        "    csv_content = file.read()\n",
        "    print(csv_content)\n",
        "\n",
        "files.download('tesla_tweets.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "u20ijWSUiJSm",
        "outputId": "74b3a4e5-c6b6-44d2-ee24-c1ff24104220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CSV Content ---\n",
            "Date,User ID,Tweet\n",
            "2025-02-11 15:09:04+00:00,1266811878879215616,\"RT @le20hfrancetele: 🚗📉 Chez Tesla, des ventes en panne sèche : elles se sont effondrées en Europe avec une baisse de 13 % sur un an.\n",
            "\n",
            "En F…\"\n",
            "2025-02-11 15:09:04+00:00,613671143,\"@elonmusk Hey @elonmusk! You should be pro work from home. The 2026 Model Y looks great but I’m not allowed to park that at the shop. It has been made clear to me that a red Ferrari 12Cilindri with a bumper sticker on the back that says, “Jingoism Is Sexy” is fine but no Tesla!\"\n",
            "2025-02-11 15:09:04+00:00,1503692354221338625,\"RT @PhDcornerHub: @niccruzpatane Here's your passage refined for punctuation and English:\n",
            "\n",
            "\"\"Why is India, alongside Southeast countries, co…\"\n",
            "2025-02-11 15:09:03+00:00,1693759037253271552,\"@BasedMikeLee @elonmusk Elon has millions of federal contracts and wants to benefit his business (tax exempt and payers) and wants more money for himself and shudder agencies that were investigating him for various things (tesla cars etc)\n",
            "\n",
            "This isn’t the same as some out of gov firm doing a pro audit.\"\n",
            "2025-02-11 15:09:03+00:00,1606315269504245762,@MrChallinger @Tesla @cybertruck Remember to come up for air when gobbling that hard 😬 https://t.co/I3z2VnpuDx\n",
            "2025-02-11 15:09:02+00:00,1585585645694631936,\"RT @politvidchannel: BREAKING: Canada Just Told Trump That if he moves forward with his tariff on Canada, They will put '100% tariff on Tes…\"\n",
            "2025-02-11 15:09:02+00:00,1737768058314481664,\"RT @politvidchannel: BREAKING: Canada Just Told Trump That if he moves forward with his tariff on Canada, They will put '100% tariff on Tes…\"\n",
            "2025-02-11 15:09:02+00:00,411200575,\"RT @politvidchannel: BREAKING: Canada Just Told Trump That if he moves forward with his tariff on Canada, They will put '100% tariff on Tes…\"\n",
            "2025-02-11 15:09:01+00:00,1161962688039116801,RT @CNBC: Michael Burry of ‘The Big Short’ reveals a $530 million bet against Tesla https://t.co/41e0YK0tiG\n",
            "2025-02-11 15:09:00+00:00,815226656914817024,RT @J_Overstreet14: Thanking Tesla for making a safe car and then immediately explaining that that car autonomously drove itself over a cur…\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_471da9a3-cbfb-43fc-86a2-4db77932d410\", \"tesla_tweets.csv\", 2166)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"(Tesla OR TSLA OR Tesla stock OR Tesla shares) -is:retweet lang:en\"\n",
        "\n",
        "# # # Fetch recent tweets (last 7 days)\n",
        "# # tweets = client.search_recent_tweets(query=query, max_results=5, tweet_fields=[\"created_at\", \"text\"])\n",
        "\n",
        "# # # Store in DataFrame & Save\n",
        "# # data = [[tweet.created_at, tweet.text] for tweet in tweets.data]\n",
        "# # df = pd.DataFrame(data, columns=[\"timestamp\", \"tweet\"])\n",
        "# # df.to_csv(\"stock_tweets.csv\", index=False)\n",
        "\n",
        "# # print(\"Saved tweets to stock_tweets.csv!\")\n",
        "# import requests\n",
        "\n",
        "# url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "\n",
        "# params = {\n",
        "#     \"query\": \"Tesla OR TSLA OR Tesla stock -is:retweet lang:en\",\n",
        "#     \"max_results\": 10,\n",
        "#     \"tweet.fields\": \"created_at,text\"\n",
        "# }\n",
        "\n",
        "# # API headers\n",
        "# headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
        "\n",
        "# # Make request\n",
        "# response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "# # Check response status\n",
        "# if response.status_code == 200:\n",
        "#     tweets = response.json()\n",
        "#     for tweet in tweets[\"data\"]:\n",
        "#         print(f\"{tweet['created_at']}: {tweet['text']}\\n\")\n",
        "# else:\n",
        "#     print(f\"Error {response.status_code}: {response.text}\")"
      ],
      "metadata": {
        "id": "pj-NmHAuiXOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsapi = NewsApiClient(api_key=\"3c53572d3893466a8240a9916ff53acb\")\n",
        "\n",
        "articles = newsapi.get_everything(q=\"Tesla stock\", language=\"en\", page_size=100)\n",
        "\n",
        "news_data=[]\n",
        "for article in articles[\"articles\"]:\n",
        "    news_data.append({'Date': article['publishedAt'], 'Title': article['title']})"
      ],
      "metadata": {
        "id": "V-oRZdWMitRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id=\"AHwBapk4BbgYfXKt4SSpAw\",\n",
        "    client_secret=\"_iiyfM51ZSRzzFtwYV3zqBxbbj7fqw\",\n",
        "    user_agent=\"StockSentimentAnalysis\"\n",
        ")\n",
        "\n",
        "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
        "posts = subreddit.search(\"Tesla stock\", limit=100)\n",
        "\n",
        "reddit_data = []\n",
        "for post in posts:\n",
        "    # post_date = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    post_date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
        "    reddit_data.append({\"Date\": post_date, \"Title\": post.title})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX0PpTmMuciY",
        "outputId": "f480a631-46e4-4224-9457-ff9d2d90a6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_df = pd.DataFrame(reddit_data)\n",
        "news_df = pd.DataFrame(news_data)\n",
        "\n",
        "# Convert Date column to datetime and remove timezones\n",
        "for df in [reddit_df, news_df]:\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True).dt.tz_convert(None).dt.date\n",
        "\n",
        "reddit_df.rename(columns={\"Title\": \"title_reddit\"}, inplace=True)\n",
        "news_df.rename(columns={\"Title\": \"title_news\"}, inplace=True)\n",
        "\n",
        "# Merge using an outer join to retain all dates\n",
        "merged_data = pd.merge(\n",
        "    reddit_df[[\"Date\", \"title_reddit\"]],\n",
        "    news_df[[\"Date\", \"title_news\"]],\n",
        "    on=\"Date\",\n",
        "    how=\"outer\"\n",
        ")\n",
        "\n",
        "# Sort by Date in place for memory efficiency\n",
        "merged_data.sort_values(\"Date\", inplace=True)\n",
        "\n",
        "print(merged_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K7U0ViiMPCx",
        "outputId": "c50fbbeb-b8fd-432e-bf99-3c585d547c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date                                       title_reddit  \\\n",
            "0    2017-11-03  Tesla stock to rebound to $400 tomorrow? DD in...   \n",
            "1    2018-07-02  WSB reaction next Tesla quarterly earnings cal...   \n",
            "2    2019-05-21  Tesla stock worth just $10 in worst case: Morg...   \n",
            "3    2019-07-02  Tesla delivers 95,200 vehicles. Stock shoots u...   \n",
            "4    2020-02-03  A request to all the owners of a Tesla (the ca...   \n",
            "..          ...                                                ...   \n",
            "193  2025-02-07  More electric cars sold in Europe, but Tesla t...   \n",
            "192  2025-02-07  Elon Musk's Brother Kimbal Musk And Other Tesl...   \n",
            "194  2025-02-09                                                NaN   \n",
            "195  2025-02-10                                                NaN   \n",
            "196  2025-02-11  Kimbal Musk sells Tesla stock worth $27.6 million   \n",
            "\n",
            "                                            title_news  \n",
            "0                                                  NaN  \n",
            "1                                                  NaN  \n",
            "2                                                  NaN  \n",
            "3                                                  NaN  \n",
            "4                                                  NaN  \n",
            "..                                                 ...  \n",
            "193  The 'Magnificent Seven' era is ending, says th...  \n",
            "192  The 'Magnificent Seven' era is ending, says th...  \n",
            "194  The Tesla Cybertruck – Once A King’s Ransom – ...  \n",
            "195     To buy a Tesla Model 3, only to end up in hell  \n",
            "196                                                NaN  \n",
            "\n",
            "[197 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tesla_df = pd.DataFrame(tesla_data).reset_index()\n",
        "\n",
        "tesla_df[\"Date\"] = pd.to_datetime(tesla_df[\"Date\"]).dt.tz_localize(None).dt.date\n",
        "\n",
        "# Merge Tesla stock data with sentiment data\n",
        "final_data = pd.merge(\n",
        "    merged_data,\n",
        "    tesla_df,\n",
        "    on=\"Date\",\n",
        "    how=\"outer\"\n",
        ")\n",
        "\n",
        "# Sort by Date for analysis\n",
        "final_data.sort_values(\"Date\", inplace=True)\n",
        "\n",
        "# Display merged dataset\n",
        "print(final_data.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlqmkGb5biMm",
        "outputId": "84cba06b-7803-47f3-d853-d71ae73c07c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                       title_reddit title_news  \\\n",
            "0  2017-11-03  Tesla stock to rebound to $400 tomorrow? DD in...        NaN   \n",
            "1  2018-07-02  WSB reaction next Tesla quarterly earnings cal...        NaN   \n",
            "2  2019-05-21  Tesla stock worth just $10 in worst case: Morg...        NaN   \n",
            "3  2019-07-02  Tesla delivers 95,200 vehicles. Stock shoots u...        NaN   \n",
            "4  2020-02-03  A request to all the owners of a Tesla (the ca...        NaN   \n",
            "\n",
            "   Open  High  Low  Close  Volume  Dividends  Stock Splits  \n",
            "0   NaN   NaN  NaN    NaN     NaN        NaN           NaN  \n",
            "1   NaN   NaN  NaN    NaN     NaN        NaN           NaN  \n",
            "2   NaN   NaN  NaN    NaN     NaN        NaN           NaN  \n",
            "3   NaN   NaN  NaN    NaN     NaN        NaN           NaN  \n",
            "4   NaN   NaN  NaN    NaN     NaN        NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.to_csv('final_data.csv', index=False)\n",
        "\n",
        "print(\"final_data saved to final_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X7JnATS7gBg",
        "outputId": "f88acfe0-649f-448d-fead-bf13d62d450e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_data saved to final_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "files.download('final_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bRNo2FH-kBvx",
        "outputId": "a958f5dd-9353-4f7f-e043-753b8d252d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e7e14d53-bd59-477e-9383-3f205b54c97e\", \"final_data.csv\", 50260)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}